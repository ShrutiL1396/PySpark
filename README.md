<h1 align="center">Data Analysis using PySpark and Hadoop</h1>

Big Data is often extremely voluminous and rich in information. However traditional Data Analysis tools often fail to handle this plethora of data which can contain Data Streams as well. In order to perform wrangling and insight gathering on such datasets we need to make use of PySpark.

## Description
This repository attempts to showcase Data Streaming using Python provided "Generators", "Iterators" and High Order Functions such as filter(), map() and reduce(). Generators can be used as Data Streams and usually do not store everything in memory, a vital requirement when dealing with Big Data. The exercises in this repository show the usage of PySpark RDDs or Resilient Distributed Dataframes which perform operations on datasets by leveraging Hadoop backed Distributed Parallel computing. Spark Data frames are also a more user-friendly mode of performing data manipulation of structured data. Machine Learning and exercises which run Spark on Google Cloud Platform's Dataproc cluster have also been covered in this repository.

## Prerequisites and Installation
 - pyspark
 ```
 pip install pyspark
 ```
 
 - SparkConf
 - SparkContext
 - Google Cloud Account
 
 ## Contents
[PySpark Exercises](https://github.com/ShrutiL1396/PySpark) <br/>


## Contact
Shruti Shivaji Lanke - <br/>
shrutilanke13@gmail.com or slanke1@student.gsu.edu <br/>
Project Link - <br/>
https://github.com/ShrutiL1396/PySpark
